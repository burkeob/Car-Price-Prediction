{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29556b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m Python version is 3.10.5\n",
      "\n",
      "\u001b[42m[ OK ]\u001b[0m numpy version 1.22.4 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m matplotlib version 3.5.2 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m sklearn version 1.1.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m pandas version 1.4.2 is installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burkeobrien/opt/anaconda3/envs/ds_env/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[42m[ OK ]\u001b[0m xgboost version 1.5.1 is installed.\n",
      "\u001b[42m[ OK ]\u001b[0m shap version 0.40.0 is installed.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.10\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.10\"):\n",
    "    print(FAIL, \"Python version 3.10 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'numpy': \"1.22.4\", 'matplotlib': \"3.5.2\",'sklearn': \"1.1.1\", \n",
    "                'pandas': \"1.4.2\",'xgboost': \"1.5.1\", 'shap': \"0.40.0\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4039efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sb\n",
    "from sklearn.model_selection import train_test_split \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa02548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/audi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed2242",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470587e",
   "metadata": {},
   "source": [
    "**Splitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38379640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: (8534, 8) (8534,)\n",
      "(2134, 8) (2134,)\n",
      "validation set: (1067, 8) (1067,)\n",
      "test set: (1067, 8) (1067,)\n"
     ]
    }
   ],
   "source": [
    "# Our dataset is large enough where we don't need to bother with KFolds\n",
    "random_state = 42\n",
    "\n",
    "y = df['price']\n",
    "X = df.loc[:, df.columns != 'price']\n",
    "\n",
    "# We're going to want to stratify on price\n",
    "y_binned = pd.qcut(df['price'], q=10)\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,\\\n",
    "                    train_size = 0.8,random_state = random_state, stratify = y_binned)\n",
    "print('training set:',X_train.shape, y_train.shape) # 80% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 20% of points are in other\n",
    "\n",
    "y_binned = pd.qcut(y_other, q=10)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,\\\n",
    "                    train_size = 0.5,random_state = random_state, stratify = y_binned)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 10% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 10% of points are in test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c00306e",
   "metadata": {},
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25097a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to group very rare models into an \"other\" category- say models that are less than 0.5% of our training data\n",
    "\n",
    "rare_models = pd.DataFrame(df['model'].value_counts() / len(df))\n",
    "rare_models = rare_models[rare_models['model'] < 0.005]\n",
    "\n",
    "df['model'] = np.where(df['model'].isin(rare_models.index), 'Other', df['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de794ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocess ColumnTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "\n",
    "# categorical features\n",
    "onehot_ftrs = ['model','fuelType','transmission']\n",
    "\n",
    "# continuous with well-defined min/max\n",
    "minmax_ftrs = ['year', 'engineSize']\n",
    "# continuous better suited for StandardScaler\n",
    "std_ftrs = ['tax', 'mileage', 'mpg']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), minmax_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs),\n",
    "        ('onehot' , OneHotEncoder(sparse=False,min_frequency = 0.01, handle_unknown = 'infrequent_if_exist'), onehot_ftrs)\n",
    "    ], remainder = 'passthrough', verbose_feature_names_out = False)\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)]) \n",
    "\n",
    "\n",
    "X_train_prep = pd.DataFrame(clf.fit_transform(X_train), columns = clf.get_feature_names_out())\n",
    "X_val_prep = pd.DataFrame(clf.transform(X_val),  columns = clf.get_feature_names_out())\n",
    "X_test_prep = pd.DataFrame(clf.transform(X_test),  columns = clf.get_feature_names_out())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2aad31f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02636512772439653\n",
      "0.033739456419868794\n",
      "0.029053420805998126\n"
     ]
    }
   ],
   "source": [
    "print(X_train_prep['model_infrequent_sklearn'].sum() / len(X_train_prep))\n",
    "print(X_test_prep['model_infrequent_sklearn'].sum() / len(X_test_prep))\n",
    "print(X_val_prep['model_infrequent_sklearn'].sum() / len(X_val_prep))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
